{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use the Amazon sentiment analysis data (Blitzer et al., 2007), where the goal is to classify text documents as expressing a positive or negative sentiment (i.e., a classification problem with two classes). We are going to focus on book reviews. To load the data, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.readers.sentiment_reader as srs\n",
    "scr = srs.SentimentCorpus(\"books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will load the data in a bag-of-words representation where rare words (occurring less than 5 times in the training data) are removed.\n",
    "\n",
    "1. Implement the Naive Bayes algorithm. Open the file `multinomial_naive_bayes.py`,  which is inside the classifiers folder. In the MultinomialNaiveBayes class you will find the train method. We have already placed some code in that file to help you get started.\n",
    "    \n",
    "2. After implementing, run Naive Bayes with the multinomial model on the Amazon dataset (sentiment classification) and report results both for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes [0 1]\n",
      "n_classes 2\n",
      "x (1600, 13989)\n",
      "y (1600, 1)\n",
      "(array([   0,    2,    4,    6,    8,   10,   12,   14,   16,   18,   20,\n",
      "         22,   24,   26,   28,   30,   32,   34,   36,   38,   40,   42,\n",
      "         44,   46,   48,   50,   52,   54,   56,   58,   60,   62,   64,\n",
      "         66,   68,   70,   72,   74,   76,   78,   80,   82,   84,   86,\n",
      "         88,   90,   92,   94,   96,   98,  100,  102,  104,  106,  108,\n",
      "        110,  112,  114,  116,  118,  120,  122,  124,  126,  128,  130,\n",
      "        132,  134,  136,  138,  140,  142,  144,  146,  148,  150,  152,\n",
      "        154,  156,  158,  160,  162,  164,  166,  168,  170,  172,  174,\n",
      "        176,  178,  180,  182,  184,  186,  188,  190,  192,  194,  196,\n",
      "        198,  200,  202,  204,  206,  208,  210,  212,  214,  216,  218,\n",
      "        220,  222,  224,  226,  228,  230,  232,  234,  236,  238,  240,\n",
      "        242,  244,  246,  248,  250,  252,  254,  256,  258,  260,  262,\n",
      "        264,  266,  268,  270,  272,  274,  276,  278,  280,  282,  284,\n",
      "        286,  288,  290,  292,  294,  296,  298,  300,  302,  304,  306,\n",
      "        308,  310,  312,  314,  316,  318,  320,  322,  324,  326,  328,\n",
      "        330,  332,  334,  336,  338,  340,  342,  344,  346,  348,  350,\n",
      "        352,  354,  356,  358,  360,  362,  364,  366,  368,  370,  372,\n",
      "        374,  376,  378,  380,  382,  384,  386,  388,  390,  392,  394,\n",
      "        396,  398,  400,  402,  404,  406,  408,  410,  412,  414,  416,\n",
      "        418,  420,  422,  424,  426,  428,  430,  432,  434,  436,  438,\n",
      "        440,  442,  444,  446,  448,  450,  452,  454,  456,  458,  460,\n",
      "        462,  464,  466,  468,  470,  472,  474,  476,  478,  480,  482,\n",
      "        484,  486,  488,  490,  492,  494,  496,  498,  500,  502,  504,\n",
      "        506,  508,  510,  512,  514,  516,  518,  520,  522,  524,  526,\n",
      "        528,  530,  532,  534,  536,  538,  540,  542,  544,  546,  548,\n",
      "        550,  552,  554,  556,  558,  560,  562,  564,  566,  568,  570,\n",
      "        572,  574,  576,  578,  580,  582,  584,  586,  588,  590,  592,\n",
      "        594,  596,  598,  600,  602,  604,  606,  608,  610,  612,  614,\n",
      "        616,  618,  620,  622,  624,  626,  628,  630,  632,  634,  636,\n",
      "        638,  640,  642,  644,  646,  648,  650,  652,  654,  656,  658,\n",
      "        660,  662,  664,  666,  668,  670,  672,  674,  676,  678,  680,\n",
      "        682,  684,  686,  688,  690,  692,  694,  696,  698,  700,  702,\n",
      "        704,  706,  708,  710,  712,  714,  716,  718,  720,  722,  724,\n",
      "        726,  728,  730,  732,  734,  736,  738,  740,  742,  744,  746,\n",
      "        748,  750,  752,  754,  756,  758,  760,  762,  764,  766,  768,\n",
      "        770,  772,  774,  776,  778,  780,  782,  784,  786,  788,  790,\n",
      "        792,  794,  796,  798,  800,  802,  804,  806,  808,  810,  812,\n",
      "        814,  816,  818,  820,  822,  824,  826,  828,  830,  832,  834,\n",
      "        836,  838,  840,  842,  844,  846,  848,  850,  852,  854,  856,\n",
      "        858,  860,  862,  864,  866,  868,  870,  872,  874,  876,  878,\n",
      "        880,  882,  884,  886,  888,  890,  892,  894,  896,  898,  900,\n",
      "        902,  904,  906,  908,  910,  912,  914,  916,  918,  920,  922,\n",
      "        924,  926,  928,  930,  932,  934,  936,  938,  940,  942,  944,\n",
      "        946,  948,  950,  952,  954,  956,  958,  960,  962,  964,  966,\n",
      "        968,  970,  972,  974,  976,  978,  980,  982,  984,  986,  988,\n",
      "        990,  992,  994,  996,  998, 1000, 1002, 1004, 1006, 1008, 1010,\n",
      "       1012, 1014, 1016, 1018, 1020, 1022, 1024, 1026, 1028, 1030, 1032,\n",
      "       1034, 1036, 1038, 1040, 1042, 1044, 1046, 1048, 1050, 1052, 1054,\n",
      "       1056, 1058, 1060, 1062, 1064, 1066, 1068, 1070, 1072, 1074, 1076,\n",
      "       1078, 1080, 1082, 1084, 1086, 1088, 1090, 1092, 1094, 1096, 1098,\n",
      "       1100, 1102, 1104, 1106, 1108, 1110, 1112, 1114, 1116, 1118, 1120,\n",
      "       1122, 1124, 1126, 1128, 1130, 1132, 1134, 1136, 1138, 1140, 1142,\n",
      "       1144, 1146, 1148, 1150, 1152, 1154, 1156, 1158, 1160, 1162, 1164,\n",
      "       1166, 1168, 1170, 1172, 1174, 1176, 1178, 1180, 1182, 1184, 1186,\n",
      "       1188, 1190, 1192, 1194, 1196, 1198, 1200, 1202, 1204, 1206, 1208,\n",
      "       1210, 1212, 1214, 1216, 1218, 1220, 1222, 1224, 1226, 1228, 1230,\n",
      "       1232, 1234, 1236, 1238, 1240, 1242, 1244, 1246, 1248, 1250, 1252,\n",
      "       1254, 1256, 1258, 1260, 1262, 1264, 1266, 1268, 1270, 1272, 1274,\n",
      "       1276, 1278, 1280, 1282, 1284, 1286, 1288, 1290, 1292, 1294, 1296,\n",
      "       1298, 1300, 1302, 1304, 1306, 1308, 1310, 1312, 1314, 1316, 1318,\n",
      "       1320, 1322, 1324, 1326, 1328, 1330, 1332, 1334, 1336, 1338, 1340,\n",
      "       1342, 1344, 1346, 1348, 1350, 1352, 1354, 1356, 1358, 1360, 1362,\n",
      "       1364, 1366, 1368, 1370, 1372, 1374, 1376, 1378, 1380, 1382, 1384,\n",
      "       1386, 1388, 1390, 1392, 1394, 1396, 1398, 1400, 1402, 1404, 1406,\n",
      "       1408, 1410, 1412, 1414, 1416, 1418, 1420, 1422, 1424, 1426, 1428,\n",
      "       1430, 1432, 1434, 1436, 1438, 1440, 1442, 1444, 1446, 1448, 1450,\n",
      "       1452, 1454, 1456, 1458, 1460, 1462, 1464, 1466, 1468, 1470, 1472,\n",
      "       1474, 1476, 1478, 1480, 1482, 1484, 1486, 1488, 1490, 1492, 1494,\n",
      "       1496, 1498, 1500, 1502, 1504, 1506, 1508, 1510, 1512, 1514, 1516,\n",
      "       1518, 1520, 1522, 1524, 1526, 1528, 1530, 1532, 1534, 1536, 1538,\n",
      "       1540, 1542, 1544, 1546, 1548, 1550, 1552, 1554, 1556, 1558, 1560,\n",
      "       1562, 1564, 1566, 1568, 1570, 1572, 1574, 1576, 1578, 1580, 1582,\n",
      "       1584, 1586, 1588, 1590, 1592, 1594, 1596, 1598]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/lukfre/DATA/phd/lxmls-toolkit/lxmls/classifiers/multinomial_naive_bayes.py:79: RuntimeWarning: divide by zero encountered in log\n",
      "/media/lukfre/DATA/phd/lxmls-toolkit/lxmls/classifiers/multinomial_naive_bayes.py:80: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "import lxmls.classifiers.multinomial_naive_bayes as mnbb\n",
    "mnb = mnbb.MultinomialNaiveBayes()\n",
    "params_nb_sc = mnb.train(scr.train_X, scr.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes [0 1]\n",
      "n_classes 2\n",
      "x (1600, 13989)\n",
      "y (1600, 1)\n",
      "0.5\n",
      "0.5\n",
      "Multinomial Naive Bayes Amazon Sentiment Accuracy train: 0.500000 test: 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/lukfre/DATA/phd/lxmls-toolkit/lxmls/classifiers/multinomial_naive_bayes.py:80: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = mnb.test(scr.train_X, params_nb_sc)\n",
    "acc_train = mnb.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = mnb.test(scr.test_X, params_nb_sc)\n",
    "acc_test = mnb.evaluate(scr.test_y, y_pred_test)\n",
    "print(\"Multinomial Naive Bayes Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that words that were not observed at training time cause problems at testtime. Why? To  solve this problem, apply a simple add-one smoothing technique:  replace the expression in Eq. 1.9 for the estimation of the conditional probabilities by\n",
    "    \n",
    "${\\hat P}(w_j|c_k) = \\frac{1+\\sum_ {m \\in \\mathcal{I}_k} n_j(x^m)}{J + \\sum_{i=1}^J \\sum_ {m\\in \\mathcal{I}_k} n_i(x^m)}.$\n",
    "\n",
    "where $J$ is the number of distinct words. This is a widely used smoothing strategy which has a Bayesian interpretation: it corresponds to choosing a uniform prior for the word distribution on both classes, and to replace the maximum likelihood criterion by a maximum a posteriori approach. This is a form of regularization, preventing the model from overfitting on the training data. See e.g. for more information. Report the new accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an implementation of the perceptron algorithm in the class Perceptron\n",
    "(file `perceptron.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands to generate a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lxmls.readers.simple_data_set as sds\n",
    "sd = sds.SimpleDataSet(\n",
    "    nr_examples=100,\n",
    "    g1=[[-1,-1],1], \n",
    "    g2=[[1,1],1], \n",
    "    balance=0.5,\n",
    "    split=[0.5,0,0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the perceptron algorithm on the simple dataset previously generated and report its train and test set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.classifiers.perceptron as percc\n",
    "perc = percc.Perceptron()\n",
    "params_perc_sd = perc.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = perc.test(sd.train_X,params_perc_sd)\n",
    "acc_train = perc.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = perc.test(sd.test_X,params_perc_sd)\n",
    "acc_test = perc.evaluate(sd.test_y, y_pred_test)\n",
    "print(\"Perceptron Simple Dataset Accuracy train: %f test: %f\"%(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = sd.plot_data(\"osx\")\n",
    "fig, axis = sd.add_line(fig, axis, params_perc_sd, \"Perceptron\", \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the code to save the intermediate weight vectors, and plot them every five iterations. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3\n",
    "We provide an implementation of the MIRA algorithm. Compare it with the perceptron for various values of $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.classifiers.mira as mirac\n",
    "mira = mirac.Mira()\n",
    "mira.regularizer = 1.0 # This is lambda\n",
    "params_mira_sd = mira.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = mira.test(sd.train_X,params_mira_sd)\n",
    "acc_train = mira.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = mira.test(sd.test_X,params_mira_sd)\n",
    "acc_test = mira.evaluate(sd.test_y, y_pred_test)\n",
    "print(\"Mira Simple Dataset Accuracy train: %f test: %f\"%(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results achieved and separating hiperplanes found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = sd.add_line(fig, axis, params_mira_sd, \"Mira\",\"green\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4\n",
    "We provide an implementation of the L-BFGS algorithm for training maximum entropy models in the class MaxEnt batch, as well as an implementation of the SGD algorithm in the class `MaxEnt online`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.classifiers.max_ent_batch as mebc\n",
    "me_lbfgs = mebc.MaxEntBatch()\n",
    "me_lbfgs.regularizer = 1.0\n",
    "params_meb_sd = me_lbfgs.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = me_lbfgs.test(sd.train_X,params_meb_sd)\n",
    "acc_train = me_lbfgs.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = me_lbfgs.test(sd.test_X,params_meb_sd)\n",
    "acc_test = me_lbfgs.evaluate(sd.test_y, y_pred_test)\n",
    "print(\n",
    "    \"Max-Ent batch Simple Dataset Accuracy train: %f test: %f\" % \n",
    "    (acc_train,acc_test)\n",
    ")\n",
    "fig, axis = sd.add_line(fig, axis, params_meb_sd, \"Max-Ent-Batch\",\"orange\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a maximum entropy model using L-BFGS, on the Amazon dataset (try different values of $\\lambda$) and report training and test set accuracy. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_meb_sc = me_lbfgs.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = me_lbfgs.test(scr.train_X,params_meb_sc)\n",
    "acc_train = me_lbfgs.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = me_lbfgs.test(scr.test_X,params_meb_sc)\n",
    "acc_test = me_lbfgs.evaluate(scr.test_y, y_pred_test)\n",
    "print(\n",
    "    \"Max-Ent Batch Amazon Sentiment Accuracy train: %f test: %f\" % \n",
    "    (acc_train, acc_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fix $\\lambda$ = 1.0 and train with SGD (you might try to adjust the initial step). Compare the objective values obtained during training with those obtained with L-BFGS. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.classifiers.max_ent_online as meoc\n",
    "me_sgd = meoc.MaxEntOnline()\n",
    "me_sgd.regularizer = 1.0\n",
    "params_meo_sc = me_sgd.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = me_sgd.test(scr.train_X,params_meo_sc)\n",
    "acc_train = me_sgd.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = me_sgd.test(scr.test_X,params_meo_sc)\n",
    "acc_test = me_sgd.evaluate(scr.test_y, y_pred_test)\n",
    "print(\n",
    "    \"Max-Ent Online Amazon Sentiment Accuracy train: %f test: %f\" % \n",
    "    (acc_train, acc_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.5\n",
    "Run the SVM primal algorithm. Then, repeat the MaxEnt exercise now using SVMs, for several values of $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.classifiers.svm as svmc\n",
    "svm = svmc.SVM()\n",
    "svm.regularizer = 1.0 # This is lambda\n",
    "params_svm_sd = svm.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = svm.test(sd.train_X,params_svm_sd)\n",
    "acc_train = svm.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = svm.test(sd.test_X,params_svm_sd)\n",
    "acc_test = svm.evaluate(sd.test_y, y_pred_test)\n",
    "print(\"SVM Online Simple Dataset Accuracy train: {} test: {}\".format(acc_train,acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results achieved and separating hyperplanes found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = sd.add_line(fig, axis, params_svm_sd, \"SVM\", \"yellow\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_svm_sc = svm.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = svm.test(scr.train_X,params_svm_sc)\n",
    "acc_train = svm.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = svm.test(scr.test_X,params_svm_sc)\n",
    "acc_test = svm.evaluate(scr.test_y, y_pred_test)\n",
    "print(\"SVM Online Amazon Sentiment Accuracy train: {} test: {}\".format(acc_train,acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
